{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuWPzNXgi2lHz1CQ81Nre2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharmesh-b/AI-for-Finance-Stocks-real-time-analysis-/blob/master/hb_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Nl8GIbSPmu"
      },
      "outputs": [],
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround to avoid following error at notebook\n",
        "# Error: NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "zRHsNCtZShyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U huggingface_hub\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain\n",
        "\n",
        "#Open-source pure-python PDF library capable of splitting, merging, cropping,\n",
        "#and transforming the pages of PDF files\n",
        "!pip install -q -U pypdf\n",
        "\n",
        "#Python framework for state-of-the-art sentence, text and image embeddings.\n",
        "!pip install -q -U sentence-transformers\n",
        "\n",
        "# FAISS Vector Databses specific Libraries\n",
        "!pip install -q -U faiss-gpu"
      ],
      "metadata": {
        "id": "0MHMgxD3VtYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "yu9Jh8DJV3qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are using Mistral 7 Billion parameter LLM (Open source from HuggingFace)**\n",
        "If you're new to Hugging Face or machine learning tasks, the following lines of code introduce some scary concepts. This Mistral 7B LLM, although big in size (approximately 14 GB), presents a challenge when loading on a Colab notebook, requiring larger GPUs and making it incompatible with the free T4 GPU.\n",
        "\n",
        "To address this issue, we employ a sharded version of the Mistral LLM. This version is loaded in smaller increments (approximately 1.9 GB each) in the notebook and subsequently aggregated into a single file. This allows us to seamlessly utilize the LLM on a free Colab GPU (T4 GPU) without incurring any costs for learning purposes.\n",
        "\n",
        "These lines of code incorporate several important concepts, which may initially seem complex but are easily comprehensible:\n",
        "\n",
        "BitsAndBytesConfig:\n",
        "load_in_4bit\n",
        "bnb_4bit_use_double_quant\n",
        "bnb_4bit_quant_type\n",
        "bnb_4bit_compute_dtype\n",
        "AutoModelForCausalLM.from_pretrained\n",
        "\n",
        "AutoTokenizer.from_pretrained\n",
        "\n",
        "Despite the use of some technical terminology, don't be discouraged. In the coming days, we'll delve into these concepts in an approachable manner, breaking down the complexities. This includes a detailed understanding of the mentioned configurations, loading procedures, and tokenization processes. Embrace the learning journey; it's simpler than it seems."
      ],
      "metadata": {
        "id": "GV2LWm2pWflU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained (origin_model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ],
      "metadata": {
        "id": "DKynGhOHWDTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating pipelines to run LLM at Colab notebook\n",
        "It uses\n",
        "\n",
        "Huggingface transformers pipeline\n",
        "This is an object that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
        "\n",
        "We are using \"text-generation\" task. It's sort of chatGPT where you ask question and model respond with answer of that question based on it's intelligence.\n",
        "\n",
        "LangChain HuggingFacePipeline\n",
        "Integrating \"transformers.pipeline\" with LangChain.\n",
        "You may be thinking why we need this. We will be using LangChain extensively later and this is first step to integrate our LLM with LangChain"
      ],
      "metadata": {
        "id": "928_GGXDZohC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,\n",
        "    temperature = 0.3,\n",
        "    do_sample=True,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "45pkCb8cZfxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using CSV File with LangChain CSVLoader"
      ],
      "metadata": {
        "id": "3kUOwlVlZyQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "# Load the CSV file\n",
        "loader = CSVLoader(file_path='/content/drive/MyDrive/Colab Notebooks/pune_house_prices.csv')\n",
        "data = loader.load()\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "#separator=\"\"\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "chunked_docs  = text_splitter.split_documents(data)\n",
        "\n",
        "# Using HuggingFace embeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "haT2yqWXZzeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(chunked_docs,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "\n",
        "# Connect query to FAISS index using a retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ],
      "metadata": {
        "id": "JdkV3QcYaCko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Conversational Retrieval Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(mistral_llm, retriever,return_source_documents=True)"
      ],
      "metadata": {
        "id": "73jBddFOaKa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will run an infinite loop to ask questions to LLM and retrieve answers untill the user wants to quit\n",
        "import sys\n",
        "chat_history = []\n",
        "query = input('Prompt: ')\n",
        "# Ask question related CSV file like Which country has highest green gas?\n",
        "result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n",
        "print('Answer: ' + result['answer'] + '\\n')\n",
        "chat_history.append((query, result['answer']))"
      ],
      "metadata": {
        "id": "ZUpJtlwAaQD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}